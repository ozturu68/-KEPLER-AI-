#!/usr/bin/env python3
"""
Feature scaling scripti.

Bu script, preprocessed veriyi (train/val/test) scale eder ve
data/scaled/ dizinine kaydeder.

KullanÄ±m:
    python scripts/scale_features.py

    # FarklÄ± scaler ile:
    python scripts/scale_features.py --method standard

    # Veya Makefile ile:
    make scale-features
"""

import argparse
import sys
from pathlib import Path

# Proje kÃ¶kÃ¼nÃ¼ path'e ekle
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import pandas as pd
from loguru import logger

from src.core import DATA_PROCESSED, TARGET_COLUMN
from src.features.scalers import FeatureScaler, scale_train_val_test


def setup_logger():
    """Logger'Ä± yapÄ±landÄ±r."""
    logger.remove()  # Default handler'Ä± kaldÄ±r
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO",
    )


def parse_args():
    """Komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± parse et."""
    parser = argparse.ArgumentParser(description="Feature scaling scripti")

    parser.add_argument(
        "--method",
        type=str,
        default="robust",
        choices=["standard", "robust", "minmax"],
        help="Scaling yÃ¶ntemi (default: robust)",
    )

    parser.add_argument(
        "--input-dir", type=str, default=str(DATA_PROCESSED), help="Input dizini (default: data/processed)"
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default=str(DATA_PROCESSED.parent / "scaled"),
        help="Output dizini (default: data/scaled)",
    )

    return parser.parse_args()


def load_data(input_dir: Path) -> tuple:
    """
    Train/val/test verilerini yÃ¼kle.

    Args:
        input_dir: Input dizini

    Returns:
        tuple: (train_df, val_df, test_df)
    """
    logger.info("=" * 70)
    logger.info("VERÄ° YÃœKLEME")
    logger.info("=" * 70)

    train_path = input_dir / "train.csv"
    val_path = input_dir / "val.csv"
    test_path = input_dir / "test.csv"

    # Dosya kontrolÃ¼
    for path in [train_path, val_path, test_path]:
        if not path.exists():
            raise FileNotFoundError(f"Dosya bulunamadÄ±: {path}")

    # YÃ¼kle
    logger.info(f"ğŸ“‚ Input: {input_dir}")

    train_df = pd.read_csv(train_path, low_memory=False)
    logger.info(f"  âœ“ train.csv: {len(train_df):,} satÄ±r, {len(train_df.columns)} sÃ¼tun")

    val_df = pd.read_csv(val_path, low_memory=False)
    logger.info(f"  âœ“ val.csv: {len(val_df):,} satÄ±r, {len(val_df.columns)} sÃ¼tun")

    test_df = pd.read_csv(test_path, low_memory=False)
    logger.info(f"  âœ“ test.csv: {len(test_df):,} satÄ±r, {len(test_df.columns)} sÃ¼tun")

    logger.info("=" * 70)

    return train_df, val_df, test_df


def show_scaling_comparison(original_df: pd.DataFrame, scaled_df: pd.DataFrame, sample_features: list):
    """
    Scaling Ã¶ncesi ve sonrasÄ± karÅŸÄ±laÅŸtÄ±rma gÃ¶ster.

    Args:
        original_df: Orijinal DataFrame
        scaled_df: Scale edilmiÅŸ DataFrame
        sample_features: Ã–rnek feature'lar
    """
    logger.info("\n" + "=" * 70)
    logger.info("SCALING KARÅILAÅTIRMA (Ä°lk 3 Feature)")
    logger.info("=" * 70)

    comparison_data = []

    for feat in sample_features[:3]:
        if feat in original_df.columns and feat in scaled_df.columns:
            comparison_data.append(
                {
                    "Feature": feat,
                    "Original Min": f"{original_df[feat].min():.4f}",
                    "Original Max": f"{original_df[feat].max():.4f}",
                    "Original Mean": f"{original_df[feat].mean():.4f}",
                    "Scaled Min": f"{scaled_df[feat].min():.4f}",
                    "Scaled Max": f"{scaled_df[feat].max():.4f}",
                    "Scaled Mean": f"{scaled_df[feat].mean():.4f}",
                }
            )

    if comparison_data:
        import pandas as pd

        comparison_df = pd.DataFrame(comparison_data)

        # GÃ¼zel tablo formatÄ±
        for _, row in comparison_df.iterrows():
            logger.info(f"\nğŸ“Š {row['Feature']}:")
            logger.info(
                f"   Original: min={row['Original Min']}, max={row['Original Max']}, mean={row['Original Mean']}"
            )
            logger.info(f"   Scaled:   min={row['Scaled Min']}, max={row['Scaled Max']}, mean={row['Scaled Mean']}")

    logger.info("\n" + "=" * 70)


def save_data(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame, output_dir: Path):
    """
    Scaled veriyi kaydet.

    Args:
        train_df: Train DataFrame
        val_df: Validation DataFrame
        test_df: Test DataFrame
        output_dir: Output dizini
    """
    logger.info("\n" + "=" * 70)
    logger.info("VERÄ° KAYDETME")
    logger.info("=" * 70)

    # Output dizini oluÅŸtur
    output_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"ğŸ“‚ Output: {output_dir}")

    # Kaydet
    train_path = output_dir / "train_scaled.csv"
    train_df.to_csv(train_path, index=False)
    logger.info(f"  âœ“ train_scaled.csv: {len(train_df):,} satÄ±r")

    val_path = output_dir / "val_scaled.csv"
    val_df.to_csv(val_path, index=False)
    logger.info(f"  âœ“ val_scaled.csv: {len(val_df):,} satÄ±r")

    test_path = output_dir / "test_scaled.csv"
    test_df.to_csv(test_path, index=False)
    logger.info(f"  âœ“ test_scaled.csv: {len(test_df):,} satÄ±r")

    # Boyut kontrolÃ¼
    total_size = sum([p.stat().st_size for p in [train_path, val_path, test_path]])
    logger.info(f"  ğŸ“ Toplam boyut: {total_size / (1024*1024):.2f} MB")

    logger.info("=" * 70)


def main():
    """Ana fonksiyon."""
    # Setup
    setup_logger()
    args = parse_args()

    logger.info("=" * 70)
    logger.info("ğŸ”§ KEPLER EXOPLANET - FEATURE SCALING")
    logger.info("=" * 70)
    logger.info(f"ğŸ“… Tarih: 2025-11-11 16:32:09 UTC")
    logger.info(f"ğŸ‘¤ KullanÄ±cÄ±: sulegogh")
    logger.info(f"ğŸ¯ Scaling Method: {args.method}")
    logger.info("=" * 70)

    # Veriyi yÃ¼kle
    input_dir = Path(args.input_dir)
    train_df, val_df, test_df = load_data(input_dir)

    # Orijinal veriyi sakla (karÅŸÄ±laÅŸtÄ±rma iÃ§in)
    train_original = train_df.copy()

    # Scale et
    logger.info("\n" + "=" * 70)
    logger.info(f"FEATURE SCALING (method={args.method})")
    logger.info("=" * 70)

    train_scaled, val_scaled, test_scaled, scaler = scale_train_val_test(
        train_df=train_df, val_df=val_df, test_df=test_df, method=args.method, exclude_cols=[TARGET_COLUMN]
    )

    # KarÅŸÄ±laÅŸtÄ±rma gÃ¶ster
    numerical_features = [col for col in train_df.select_dtypes(include=["number"]).columns if col != TARGET_COLUMN]
    show_scaling_comparison(train_original, train_scaled, numerical_features)

    # Kaydet
    output_dir = Path(args.output_dir)
    save_data(train_scaled, val_scaled, test_scaled, output_dir)

    # Ã–zet
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š Ã–ZET")
    logger.info("=" * 70)
    logger.info(f"âœ… Scaling method: {args.method}")
    logger.info(f"âœ… Scaled features: {len(scaler.numerical_features)}")
    logger.info(f"âœ… Train: {len(train_scaled):,} satÄ±r")
    logger.info(f"âœ… Val: {len(val_scaled):,} satÄ±r")
    logger.info(f"âœ… Test: {len(test_scaled):,} satÄ±r")
    logger.info(f"ğŸ“‚ Output: {output_dir}")
    logger.info("=" * 70)
    logger.info("ğŸ‰ Feature scaling tamamlandÄ±!")
    logger.info("\nğŸš€ Sonraki adÄ±m: Feature Engineering (polynomial features)")
    logger.info("=" * 70)

    return 0


if __name__ == "__main__":
    sys.exit(main())
