#!/usr/bin/env python3
"""
Feature selection scripti.

Bu script, engineered veriyi alÄ±r, en iyi feature'larÄ± seÃ§er ve
data/selected/ dizinine kaydeder.

KullanÄ±m:
    python scripts/select_features.py
    
    # FarklÄ± feature sayÄ±sÄ± ile:
    python scripts/select_features.py --n-features 40
    
    # Veya Makefile ile:
    make select-features
"""

import sys
import argparse
from pathlib import Path

# Proje kÃ¶kÃ¼nÃ¼ path'e ekle
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from loguru import logger

from src.core import TARGET_COLUMN
from src.features.selection import FeatureSelector, select_features_train_val_test


def setup_logger():
    """Logger'Ä± yapÄ±landÄ±r."""
    logger.remove()  # Default handler'Ä± kaldÄ±r
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO"
    )


def parse_args():
    """Komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± parse et."""
    parser = argparse.ArgumentParser(description="Feature selection scripti")
    
    parser.add_argument(
        "--input-dir",
        type=str,
        default="data/engineered",
        help="Input dizini (default: data/engineered)"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/selected",
        help="Output dizini (default: data/selected)"
    )
    
    parser.add_argument(
        "--n-features",
        type=int,
        default=50,
        help="SeÃ§ilecek feature sayÄ±sÄ± (default: 50)"
    )
    
    parser.add_argument(
        "--method",
        type=str,
        default="auto",
        choices=["auto", "correlation", "importance", "hybrid"],
        help="Selection yÃ¶ntemi (default: auto)"
    )
    
    parser.add_argument(
        "--save-plots",
        action="store_true",
        help="Feature importance grafiklerini kaydet"
    )
    
    return parser.parse_args()


def load_data(input_dir: Path) -> tuple:
    """
    Train/val/test verilerini yÃ¼kle.
    
    Args:
        input_dir: Input dizini
        
    Returns:
        tuple: (train_df, val_df, test_df)
    """
    logger.info("="*70)
    logger.info("VERÄ° YÃœKLEME")
    logger.info("="*70)
    
    train_path = input_dir / "train_engineered.csv"
    val_path = input_dir / "val_engineered.csv"
    test_path = input_dir / "test_engineered.csv"
    
    # Dosya kontrolÃ¼
    for path in [train_path, val_path, test_path]:
        if not path.exists():
            raise FileNotFoundError(f"Dosya bulunamadÄ±: {path}")
    
    # YÃ¼kle
    logger.info(f"ğŸ“‚ Input: {input_dir}")
    
    train_df = pd.read_csv(train_path, low_memory=False)
    logger.info(f"  âœ“ train_engineered.csv: {len(train_df):,} satÄ±r, {len(train_df.columns)} sÃ¼tun")
    
    val_df = pd.read_csv(val_path, low_memory=False)
    logger.info(f"  âœ“ val_engineered.csv: {len(val_df):,} satÄ±r, {len(val_df.columns)} sÃ¼tun")
    
    test_df = pd.read_csv(test_path, low_memory=False)
    logger.info(f"  âœ“ test_engineered.csv: {len(test_df):,} satÄ±r, {len(test_df.columns)} sÃ¼tun")
    
    logger.info("="*70)
    
    return train_df, val_df, test_df


def show_selection_summary(selection_info: dict):
    """
    Feature selection Ã¶zetini gÃ¶ster.
    
    Args:
        selection_info: Selection bilgileri
    """
    logger.info("\n" + "="*70)
    logger.info("SELECTION Ã–ZETÄ°")
    logger.info("="*70)
    
    logger.info(f"ğŸ“Š Ä°lk feature sayÄ±sÄ±: {selection_info['initial_features']}")
    logger.info(f"ğŸ“Š Filtreleme sonrasÄ±: {selection_info['after_filtering']}")
    logger.info(f"ğŸ“Š SeÃ§ilen feature sayÄ±sÄ±: {selection_info['selected_features']}")
    
    logger.info(f"\nğŸ—‘ï¸  KaldÄ±rÄ±lan feature'lar:")
    logger.info(f"   - DÃ¼ÅŸÃ¼k varyans: {selection_info['dropped_low_variance']}")
    logger.info(f"   - YÃ¼ksek korelasyon: {selection_info['dropped_high_correlation']}")
    logger.info(f"   - DÃ¼ÅŸÃ¼k importance: {selection_info['dropped_low_importance']}")
    logger.info(f"   - TOPLAM: {selection_info['initial_features'] - selection_info['selected_features']}")
    
    logger.info(f"\nğŸ¯ Top 10 Features (Importance):")
    importance_df = selection_info['importance_df']
    for idx, row in importance_df.head(10).iterrows():
        logger.info(f"   {idx+1:2d}. {row['feature']:40s} â†’ {row['importance']:.4f}")
    
    logger.info("="*70)


def plot_feature_importance(importance_df: pd.DataFrame, output_dir: Path, top_n: int = 30):
    """
    Feature importance grafiÄŸi Ã§iz ve kaydet.
    
    Args:
        importance_df: Feature importance DataFrame
        output_dir: Ã‡Ä±ktÄ± dizini
        top_n: GÃ¶sterilecek feature sayÄ±sÄ±
    """
    logger.info(f"\nğŸ“Š Feature importance grafiÄŸi Ã§iziliyor (top {top_n})...")
    
    # Top N al
    top_features = importance_df.head(top_n)
    
    # Plot
    plt.figure(figsize=(12, 10))
    
    # Horizontal bar plot
    plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importance', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.title(f'Top {top_n} Feature Importance (Random Forest)', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()  # En Ã¶nemli Ã¼stte
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    
    # Kaydet
    output_path = output_dir / "feature_importance.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    logger.info(f"  âœ“ Grafik kaydedildi: {output_path}")


def show_dropped_features(selector: FeatureSelector):
    """
    KaldÄ±rÄ±lan feature'larÄ± gÃ¶ster.
    
    Args:
        selector: FeatureSelector objesi
    """
    logger.info("\n" + "="*70)
    logger.info("KALDIRILAN FEATURES")
    logger.info("="*70)
    
    if selector.dropped_features.get('low_variance'):
        logger.info(f"\nğŸ“‰ DÃ¼ÅŸÃ¼k Varyans ({len(selector.dropped_features['low_variance'])} adet):")
        for feat in selector.dropped_features['low_variance'][:10]:
            logger.info(f"   - {feat}")
        if len(selector.dropped_features['low_variance']) > 10:
            logger.info(f"   ... ve {len(selector.dropped_features['low_variance']) - 10} tane daha")
    
    if selector.dropped_features.get('high_correlation'):
        logger.info(f"\nğŸ”— YÃ¼ksek Korelasyon ({len(selector.dropped_features['high_correlation'])} adet):")
        for feat in selector.dropped_features['high_correlation'][:10]:
            logger.info(f"   - {feat}")
        if len(selector.dropped_features['high_correlation']) > 10:
            logger.info(f"   ... ve {len(selector.dropped_features['high_correlation']) - 10} tane daha")
    
    logger.info("="*70)


def save_data(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame, 
              output_dir: Path, selected_features: list):
    """
    Selected veriyi kaydet.
    
    Args:
        train_df: Train DataFrame
        val_df: Validation DataFrame
        test_df: Test DataFrame
        output_dir: Output dizini
        selected_features: SeÃ§ilen feature'lar
    """
    logger.info("\n" + "="*70)
    logger.info("VERÄ° KAYDETME")
    logger.info("="*70)
    
    # Output dizini oluÅŸtur
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸ“‚ Output: {output_dir}")
    
    # Kaydet
    train_path = output_dir / "train_selected.csv"
    train_df.to_csv(train_path, index=False)
    logger.info(f"  âœ“ train_selected.csv: {len(train_df):,} satÄ±r, {len(train_df.columns)} sÃ¼tun")
    
    val_path = output_dir / "val_selected.csv"
    val_df.to_csv(val_path, index=False)
    logger.info(f"  âœ“ val_selected.csv: {len(val_df):,} satÄ±r, {len(val_df.columns)} sÃ¼tun")
    
    test_path = output_dir / "test_selected.csv"
    test_df.to_csv(test_path, index=False)
    logger.info(f"  âœ“ test_selected.csv: {len(test_df):,} satÄ±r, {len(test_df.columns)} sÃ¼tun")
    
    # Feature listesini kaydet
    features_path = output_dir / "selected_features.txt"
    with open(features_path, 'w') as f:
        f.write(f"# Selected Features ({len(selected_features)} adet)\n")
        f.write(f"# Date: 2025-11-11 16:42:57 UTC\n")
        f.write(f"# User: sulegogh\n\n")
        for feat in selected_features:
            f.write(f"{feat}\n")
    logger.info(f"  âœ“ selected_features.txt: {len(selected_features)} feature")
    
    # Boyut kontrolÃ¼
    total_size = sum([p.stat().st_size for p in [train_path, val_path, test_path]])
    logger.info(f"  ğŸ“ Toplam boyut: {total_size / (1024*1024):.2f} MB")
    
    logger.info("="*70)


def main():
    """Ana fonksiyon."""
    # Setup
    setup_logger()
    args = parse_args()
    
    logger.info("="*70)
    logger.info("ğŸ”§ KEPLER EXOPLANET - FEATURE SELECTION")
    logger.info("="*70)
    logger.info(f"ğŸ“… Tarih: 2025-11-11 16:42:57 UTC")
    logger.info(f"ğŸ‘¤ KullanÄ±cÄ±: sulegogh")
    logger.info(f"ğŸ¯ Target feature sayÄ±sÄ±: {args.n_features}")
    logger.info(f"ğŸ¯ Selection method: {args.method}")
    logger.info("="*70)
    
    # Veriyi yÃ¼kle
    input_dir = Path(args.input_dir)
    train_df, val_df, test_df = load_data(input_dir)
    
    # Feature selection
    logger.info("\n" + "="*70)
    logger.info("FEATURE SELECTION BAÅLADI")
    logger.info("="*70)
    
    train_selected, val_selected, test_selected, selector, selection_info = select_features_train_val_test(
        train_df=train_df,
        val_df=val_df,
        test_df=test_df,
        n_features=args.n_features,
        method=args.method
    )
    
    # Ã–zet gÃ¶ster
    show_selection_summary(selection_info)
    show_dropped_features(selector)
    
    # Grafik Ã§iz (opsiyonel)
    if args.save_plots:
        output_dir = Path(args.output_dir)
        plot_feature_importance(selection_info['importance_df'], output_dir, top_n=30)
    
    # Kaydet
    output_dir = Path(args.output_dir)
    save_data(train_selected, val_selected, test_selected, output_dir, selector.selected_features)
    
    # Final Ã¶zet
    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š FÄ°NAL Ã–ZET")
    logger.info("="*70)
    logger.info(f"âœ… Orijinal feature sayÄ±sÄ±: {selection_info['initial_features']}")
    logger.info(f"âœ… SeÃ§ilen feature sayÄ±sÄ±: {selection_info['selected_features']}")
    logger.info(f"âœ… Reduction: {(1 - selection_info['selected_features']/selection_info['initial_features'])*100:.1f}%")
    logger.info(f"âœ… Train: {len(train_selected):,} satÄ±r")
    logger.info(f"âœ… Val: {len(val_selected):,} satÄ±r")
    logger.info(f"âœ… Test: {len(test_selected):,} satÄ±r")
    logger.info(f"ğŸ“‚ Output: {output_dir}")
    logger.info("="*70)
    logger.info("ğŸ‰ Feature selection tamamlandÄ±!")
    logger.info("\nğŸš€ Sonraki adÄ±m: Model Training (CatBoost, LightGBM, XGBoost)")
    logger.info("="*70)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())