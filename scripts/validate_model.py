#!/usr/bin/env python3
"""
Model Validation Script
Quick sanity checks for model health

Author: sulegogh
Date: 2025-11-12
Version: 1.0
"""

import sys

sys.path.insert(0, '.')

from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score

from src.core.label_utils import encode_labels, validate_labels

# Import our utilities
from src.models.model_loader import ModelLoadError, load_catboost_model_v2


def validate_model(model_path: str, test_data_path: str) -> bool:
    """
    Run comprehensive model validation

    Returns:
        True if all checks pass, False otherwise
    """
    print("=" * 70)
    print("MODEL VALIDATION SCRIPT")
    print("=" * 70)

    all_passed = True

    # CHECK 1: Model loading
    print("\nüì¶ CHECK 1: Model Loading")
    try:
        model, metadata = load_catboost_model_v2(model_path)
        print("   ‚úÖ Model loaded successfully")
        print(f"   ‚ÑπÔ∏è  Created: {metadata['created_at']}")
        print(f"   ‚ÑπÔ∏è  Features: {len(metadata['feature_names'])}")
    except Exception as e:
        print(f"   ‚ùå Model loading failed: {str(e)}")
        return False

    # CHECK 2: Model methods
    print("\nüîß CHECK 2: Model Methods")
    has_predict = hasattr(model, 'predict')
    has_predict_proba = hasattr(model, 'predict_proba')

    if has_predict:
        print("   ‚úÖ predict() method exists")
    else:
        print("   ‚ùå predict() method missing")
        all_passed = False

    if has_predict_proba:
        print("   ‚úÖ predict_proba() method exists")
    else:
        print("   ‚ö†Ô∏è  predict_proba() method missing")

    # CHECK 3: Test data loading
    print("\nüìä CHECK 3: Test Data Loading")
    try:
        df_test = pd.read_csv(test_data_path)
        print(f"   ‚úÖ Test data loaded: {df_test.shape}")

        X_test = df_test.drop(columns=['koi_disposition'])
        y_test_raw = df_test['koi_disposition'].values

        print(f"   ‚ÑπÔ∏è  Features: {X_test.shape[1]}")
        print(f"   ‚ÑπÔ∏è  Samples: {len(X_test)}")

    except Exception as e:
        print(f"   ‚ùå Test data loading failed: {str(e)}")
        return False

    # CHECK 4: Feature count match
    print("\nüî¢ CHECK 4: Feature Count Match")
    expected_features = len(metadata['feature_names'])
    actual_features = X_test.shape[1]

    if expected_features == actual_features:
        print(f"   ‚úÖ Feature count matches: {expected_features}")
    else:
        print(f"   ‚ùå Feature mismatch: expected {expected_features}, got {actual_features}")
        all_passed = False

    # CHECK 5: Label format
    print("\nüè∑Ô∏è  CHECK 5: Label Format")
    print(f"   ‚ÑπÔ∏è  Label dtype: {y_test_raw.dtype}")
    print(f"   ‚ÑπÔ∏è  Unique labels: {np.unique(y_test_raw)}")

    try:
        validate_labels(y_test_raw)
        print("   ‚úÖ Labels are valid")
    except ValueError as e:
        print(f"   ‚ö†Ô∏è  Label validation warning: {str(e)}")

    # Normalize labels
    y_test = encode_labels(y_test_raw)
    print(f"   ‚úÖ Labels normalized to numeric: {np.unique(y_test)}")

    # CHECK 6: Prediction test
    print("\nüîÆ CHECK 6: Prediction Test")
    try:
        # Predict on small sample
        X_sample = X_test.head(10)
        predictions_raw = model.predict(X_sample)

        # Flatten if needed
        if len(predictions_raw.shape) > 1:
            predictions_raw = predictions_raw.flatten()

        print(f"   ‚úÖ Predictions generated: {predictions_raw.shape}")
        print(f"   ‚ÑπÔ∏è  Prediction dtype: {predictions_raw.dtype}")
        print(f"   ‚ÑπÔ∏è  Unique predictions (raw): {np.unique(predictions_raw)}")

        # Normalize predictions
        predictions = encode_labels(predictions_raw)
        print(f"   ‚úÖ Predictions normalized to numeric: {np.unique(predictions)}")
        print(f"   ‚ÑπÔ∏è  Predictions dtype after encoding: {predictions.dtype}")

    except Exception as e:
        print(f"   ‚ùå Prediction failed: {str(e)}")
        all_passed = False
        return all_passed

    # CHECK 7: Full test set prediction
    print("\nüìà CHECK 7: Full Test Set Prediction")
    try:
        predictions_raw = model.predict(X_test)

        # Flatten if needed
        if len(predictions_raw.shape) > 1:
            predictions_raw = predictions_raw.flatten()

        # Encode to numeric
        predictions = encode_labels(predictions_raw)

        # Debug info
        print(f"   ‚ÑπÔ∏è  y_test shape: {y_test.shape}, dtype: {y_test.dtype}")
        print(f"   ‚ÑπÔ∏è  predictions shape: {predictions.shape}, dtype: {predictions.dtype}")
        print(f"   ‚ÑπÔ∏è  y_test sample: {y_test[:3]}")
        print(f"   ‚ÑπÔ∏è  predictions sample: {predictions[:3]}")

        accuracy = accuracy_score(y_test, predictions)

        print(f"   ‚úÖ Full predictions successful")
        print(f"   ‚ÑπÔ∏è  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

        # Accuracy threshold check
        if accuracy >= 0.80:
            print(f"   ‚úÖ Accuracy meets threshold (‚â•0.80)")
        else:
            print(f"   ‚ùå Accuracy below threshold: {accuracy:.4f} < 0.80")
            all_passed = False

    except Exception as e:
        print(f"   ‚ùå Full prediction failed: {str(e)}")
        all_passed = False
        return all_passed

    # CHECK 8: CANDIDATE recall
    print("\nüéØ CHECK 8: CANDIDATE Recall")
    try:
        candidate_mask = y_test == 0
        candidate_predictions = predictions[candidate_mask]
        candidate_recall = (candidate_predictions == 0).mean()

        print(f"   ‚ÑπÔ∏è  CANDIDATE Recall: {candidate_recall:.4f} ({candidate_recall*100:.2f}%)")

        if candidate_recall >= 0.70:
            print(f"   ‚úÖ CANDIDATE recall meets threshold (‚â•0.70)")
        else:
            print(f"   ‚ùå CANDIDATE recall below threshold: {candidate_recall:.4f} < 0.70")
            all_passed = False

    except Exception as e:
        print(f"   ‚ùå CANDIDATE recall check failed: {str(e)}")
        all_passed = False

    # CHECK 9: Confidence analysis
    print("\nüìä CHECK 9: Confidence Analysis")
    try:
        probabilities = model.predict_proba(X_test)
        max_probs = probabilities.max(axis=1)

        mean_conf = max_probs.mean()
        low_conf_pct = (max_probs < 0.6).mean() * 100

        print(f"   ‚ÑπÔ∏è  Mean confidence: {mean_conf:.4f}")
        print(f"   ‚ÑπÔ∏è  Low confidence (<0.6): {low_conf_pct:.2f}%")

        if mean_conf >= 0.75:
            print(f"   ‚úÖ Mean confidence acceptable (‚â•0.75)")
        else:
            print(f"   ‚ö†Ô∏è  Low mean confidence: {mean_conf:.4f}")

        if low_conf_pct < 15:
            print(f"   ‚úÖ Low confidence rate acceptable (<15%)")
        else:
            print(f"   ‚ö†Ô∏è  High low-confidence rate: {low_conf_pct:.2f}%")

    except Exception as e:
        print(f"   ‚ö†Ô∏è  Confidence analysis failed: {str(e)}")

    # FINAL RESULT
    print("\n" + "=" * 70)
    if all_passed:
        print("üéâ ALL CRITICAL CHECKS PASSED")
        print("‚úÖ Model is healthy and ready for use")
    else:
        print("‚ö†Ô∏è  SOME CHECKS FAILED")
        print("‚ùå Model needs attention")
    print("=" * 70)

    return all_passed


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Validate CatBoost model")
    parser.add_argument('--model', default='models/v2_final/catboost_v2_final.pkl', help='Path to model file')
    parser.add_argument('--test-data', default='data/selected/test_selected.csv', help='Path to test data')

    args = parser.parse_args()

    success = validate_model(args.model, args.test_data)

    sys.exit(0 if success else 1)
