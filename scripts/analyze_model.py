#!/usr/bin/env python3
"""
Model analiz scripti - DetaylÄ± performans ve feature analizi.

Bu script mevcut CatBoost modelini detaylÄ± ÅŸekilde analiz eder:
- Feature importance (top 20)
- Confusion matrix
- Class-wise metrics
- Error analysis
- Confidence analysis
- Overfitting kontrolÃ¼

Usage:
    python scripts/analyze_model.py

Author: sulegogh
Date: 2025-11-11
"""

import sys
from pathlib import Path

# Proje kÃ¶kÃ¼nÃ¼ path'e ekle
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import numpy as np
import pandas as pd
from loguru import logger
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score

from src.models import CatBoostModel


def setup_logger():
    """Logger'Ä± yapÄ±landÄ±r."""
    logger.remove()
    logger.add(
        sys.stdout,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO",
    )


def load_latest_model():
    """En son kaydedilen model'i yÃ¼kle."""
    models_dir = Path("models")
    model_files = list(models_dir.glob("catboost_model_*.pkl"))

    if not model_files:
        raise FileNotFoundError("âŒ Model dosyasÄ± bulunamadÄ±!")

    # En yeni model'i seÃ§
    latest_model = max(model_files, key=lambda p: p.stat().st_mtime)
    logger.info(f"ğŸ“‚ Model yÃ¼kleniyor: {latest_model.name}")

    model = CatBoostModel()
    model.load(latest_model)

    return model, latest_model


def load_data():
    """Test verilerini yÃ¼kle."""
    logger.info("ğŸ“‚ Test verileri yÃ¼kleniyor...")

    train_df = pd.read_csv("data/selected/train_selected.csv")
    val_df = pd.read_csv("data/selected/val_selected.csv")
    test_df = pd.read_csv("data/selected/test_selected.csv")

    # NaN temizle
    for name, df in [("Train", train_df), ("Val", val_df), ("Test", test_df)]:
        nan_count = df.isnull().sum().sum()
        if nan_count > 0:
            logger.warning(f"   âš ï¸  {name}: {nan_count} NaN bulundu, 0 ile doldruluyor...")
            df.fillna(0, inplace=True)
        else:
            logger.info(f"   âœ“ {name}: NaN yok")

    return train_df, val_df, test_df


def analyze_feature_importance(model, top_n=20):
    """Feature importance detaylÄ± analiz."""
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š FEATURE IMPORTANCE ANALÄ°ZÄ°")
    logger.info("=" * 70)

    importance_df = model.get_feature_importance()

    # Top N features
    logger.info(f"\nğŸ† Top {top_n} En Ã–nemli Features:")
    for idx, row in importance_df.head(top_n).iterrows():
        logger.info(f"   {idx+1:2d}. {row['feature']:35s} â†’ {row['importance']:.6f}")

    # Ä°statistikler
    logger.info(f"\nğŸ“ˆ KÃ¼mÃ¼latif KatkÄ±:")
    logger.info(f"   Top 10: {importance_df.head(10)['importance'].sum():.2%}")
    logger.info(f"   Top 20: {importance_df.head(20)['importance'].sum():.2%}")
    logger.info(f"   Top 30: {importance_df.head(30)['importance'].sum():.2%}")

    return importance_df


def analyze_predictions(model, X, y, dataset_name="Dataset"):
    """Tahmin analizi - Confusion Matrix ve Class-wise Metrics."""
    logger.info(f"\n" + "=" * 70)
    logger.info(f"ğŸ” {dataset_name.upper()} TAHMÄ°N ANALÄ°ZÄ°")
    logger.info("=" * 70)

    # Tahminler
    y_pred = model.predict(X)
    y_pred_proba = model.predict_proba(X)

    # Confusion Matrix
    cm = confusion_matrix(y, y_pred)
    labels = sorted(y.unique())

    logger.info(f"\nğŸ“Š Confusion Matrix:")
    logger.info(f"\n{'':20s} " + " ".join([f"{lbl:>15s}" for lbl in labels]))
    logger.info("-" * 70)
    for i, true_label in enumerate(labels):
        row_str = f"{true_label:20s} "
        row_str += " ".join([f"{cm[i][j]:>15d}" for j in range(len(labels))])
        logger.info(row_str)

    # Class-wise metrics
    logger.info(f"\nğŸ“ˆ Class-wise Metrics:")
    report = classification_report(y, y_pred, output_dict=True, zero_division=0)

    for cls in labels:
        if cls in report:
            metrics = report[cls]
            logger.info(f"\n   ğŸ“Œ {cls}:")
            logger.info(f"      Precision: {metrics['precision']:.4f}")
            logger.info(f"      Recall:    {metrics['recall']:.4f}")
            logger.info(f"      F1-Score:  {metrics['f1-score']:.4f}")
            logger.info(f"      Support:   {int(metrics['support'])}")

    # Confidence Analizi
    logger.info(f"\nğŸ¯ Confidence Analizi:")
    max_proba = y_pred_proba.max(axis=1)
    logger.info(f"   Ortalama confidence: {max_proba.mean():.4f}")
    logger.info(f"   Median confidence:   {np.median(max_proba):.4f}")
    logger.info(f"   Min confidence:      {max_proba.min():.4f}")
    logger.info(f"   Max confidence:      {max_proba.max():.4f}")

    # DÃ¼ÅŸÃ¼k confidence tahminler
    low_conf_threshold = 0.6
    low_conf_count = (max_proba < low_conf_threshold).sum()
    logger.info(
        f"   DÃ¼ÅŸÃ¼k confidence (<{low_conf_threshold}): {low_conf_count} ({low_conf_count/len(max_proba)*100:.2f}%)"
    )

    return y_pred, y_pred_proba, cm


def analyze_errors(y_true, y_pred, top_n=10):
    """
    Hata analizi - yanlÄ±ÅŸ tahminleri detaylÄ± incele.

    Args:
        y_true: GerÃ§ek labels (pandas Series veya numpy array)
        y_pred: Tahmin edilen labels (numpy array)
        top_n: En sÄ±k hata yapÄ±lan kaÃ§ class Ã§iftini gÃ¶ster

    Returns:
        None
    """
    logger.info(f"\n" + "=" * 70)
    logger.info(f"âŒ HATA ANALÄ°ZÄ°")
    logger.info("=" * 70)

    # Type ve shape kontrolÃ¼
    # y_pred numpy array ise ve 2D ise flatten et
    if isinstance(y_pred, np.ndarray):
        if y_pred.ndim == 2:
            logger.debug(f"   y_pred 2D array tespit edildi: {y_pred.shape} â†’ flatten")
            y_pred = y_pred.ravel()
        elif y_pred.ndim > 2:
            raise ValueError(f"y_pred Ã§ok fazla boyutlu: {y_pred.shape}")

    # y_true pandas Series ise numpy array'e Ã§evir
    if hasattr(y_true, "values"):
        y_true_arr = y_true.values
    else:
        y_true_arr = np.array(y_true)

    # EÄŸer y_true_arr da 2D ise flatten et
    if y_true_arr.ndim == 2:
        logger.debug(f"   y_true 2D array tespit edildi: {y_true_arr.shape} â†’ flatten")
        y_true_arr = y_true_arr.ravel()

    # Shape kontrolÃ¼
    if y_true_arr.shape != y_pred.shape:
        raise ValueError(
            f"y_true ve y_pred shape'leri uyuÅŸmuyor! " f"y_true: {y_true_arr.shape}, y_pred: {y_pred.shape}"
        )

    logger.debug(f"   y_true shape: {y_true_arr.shape}")
    logger.debug(f"   y_pred shape: {y_pred.shape}")

    # YanlÄ±ÅŸ tahminler
    errors = y_true_arr != y_pred
    error_count = errors.sum()
    total_count = len(y_true_arr)

    logger.info(f"\nğŸ“‰ Hata Ä°statistikleri:")
    logger.info(f"   Toplam Ã¶rnek:  {total_count:,}")
    logger.info(f"   DoÄŸru tahmin:  {total_count - error_count:,} ({(total_count-error_count)/total_count*100:.2f}%)")
    logger.info(f"   YanlÄ±ÅŸ tahmin: {error_count:,} ({error_count/total_count*100:.2f}%)")

    if error_count == 0:
        logger.info("\n   âœ… HiÃ§ hata yok! (MÃ¼kemmel tahmin)")
        return

    # Hata daÄŸÄ±lÄ±mÄ± DataFrame
    error_indices = np.where(errors)[0]
    error_df = pd.DataFrame({"true": y_true_arr[errors], "pred": y_pred[errors], "index": error_indices})

    # En sÄ±k karÄ±ÅŸtÄ±rÄ±lan class Ã§iftleri
    logger.info(f"\nğŸ“Š En SÄ±k KarÄ±ÅŸtÄ±rÄ±lan Class Ã‡iftleri (True â†’ Predicted):")
    error_counts = error_df.groupby(["true", "pred"]).size().sort_values(ascending=False)

    logger.info(f"\n   {'#':<4s} {'True Label':<20s} {'â†’':^3s} {'Predicted Label':<20s} {'Count':>7s} {'Percent':>8s}")
    logger.info(f"   {'-'*70}")

    for i, ((true_cls, pred_cls), count) in enumerate(error_counts.head(top_n).items(), 1):
        pct = count / error_count * 100
        logger.info(f"   {i:<4d} {true_cls:<20s} {'â†’':^3s} {pred_cls:<20s} {count:>7,d} {pct:>7.1f}%")

    # Class-wise hata analizi
    logger.info(f"\nğŸ“ˆ Class-wise Hata DaÄŸÄ±lÄ±mÄ±:")
    unique_classes = np.unique(y_true_arr)

    logger.info(f"\n   {'Class':<20s} {'Total':>8s} {'Errors':>8s} {'Error Rate':>12s}")
    logger.info(f"   {'-'*55}")

    for cls in sorted(unique_classes):
        cls_mask = y_true_arr == cls
        cls_total = cls_mask.sum()
        cls_errors = (errors & cls_mask).sum()
        cls_error_rate = cls_errors / cls_total * 100 if cls_total > 0 else 0

        logger.info(f"   {cls:<20s} {cls_total:>8,d} {cls_errors:>8,d} {cls_error_rate:>11.2f}%")

    # En problemli Ã¶rnekler (hata oranÄ± en yÃ¼ksek class)
    logger.info(f"\nğŸ¯ En Problemli Class:")
    class_error_rates = []
    for cls in unique_classes:
        cls_mask = y_true_arr == cls
        cls_total = cls_mask.sum()
        cls_errors = (errors & cls_mask).sum()
        cls_error_rate = cls_errors / cls_total if cls_total > 0 else 0
        class_error_rates.append((cls, cls_error_rate, cls_errors, cls_total))

    # En yÃ¼ksek hata oranÄ±na gÃ¶re sÄ±rala
    class_error_rates.sort(key=lambda x: x[1], reverse=True)

    worst_class, worst_rate, worst_errors, worst_total = class_error_rates[0]
    logger.info(f"   Class: {worst_class}")
    logger.info(f"   Hata oranÄ±: {worst_rate*100:.2f}%")
    logger.info(f"   YanlÄ±ÅŸ tahmin: {worst_errors}/{worst_total}")

    # Bu class iÃ§in en sÄ±k karÄ±ÅŸtÄ±rÄ±lan hedef
    worst_class_errors = error_df[error_df["true"] == worst_class]
    if len(worst_class_errors) > 0:
        most_confused = worst_class_errors["pred"].value_counts().iloc[0]
        most_confused_class = worst_class_errors["pred"].value_counts().index[0]
        logger.info(f"   En Ã§ok karÄ±ÅŸtÄ±rÄ±lan: {most_confused_class} ({most_confused} kez)")


def compare_datasets(train_metrics, val_metrics, test_metrics):
    """Dataset'leri karÅŸÄ±laÅŸtÄ±r ve overfitting analizi."""
    logger.info(f"\n" + "=" * 70)
    logger.info(f"ğŸ“Š DATASET KARÅILAÅTIRMASI")
    logger.info("=" * 70)

    # Tablo
    logger.info(f"\n{'Metric':<15s} {'Train':>10s} {'Val':>10s} {'Test':>10s}")
    logger.info("-" * 50)
    logger.info(
        f"{'Accuracy':<15s} {train_metrics['acc']:>10.4f} {val_metrics['acc']:>10.4f} {test_metrics['acc']:>10.4f}"
    )
    logger.info(
        f"{'F1 Score':<15s} {train_metrics['f1']:>10.4f} {val_metrics['f1']:>10.4f} {test_metrics['f1']:>10.4f}"
    )

    # Overfitting analizi
    train_val_diff = train_metrics["acc"] - val_metrics["acc"]
    val_test_diff = val_metrics["acc"] - test_metrics["acc"]

    logger.info(f"\nğŸ” Overfitting Analizi:")
    logger.info(f"   Train-Val fark:  {train_val_diff:+.4f} ({train_val_diff*100:+.2f}%)")
    logger.info(f"   Val-Test fark:   {val_test_diff:+.4f} ({val_test_diff*100:+.2f}%)")

    if train_val_diff < 0.05:
        logger.info(f"   âœ… Model dengeli (fark < 5%)")
    elif train_val_diff < 0.10:
        logger.warning(f"   âš ï¸  Hafif overfitting (fark 5-10%)")
    else:
        logger.error(f"   âŒ Ciddi overfitting (fark > 10%)")


def main():
    """Ana fonksiyon."""
    setup_logger()

    logger.info("=" * 70)
    logger.info("ğŸ”¬ CATBOOST MODEL DETAYLI ANALÄ°Z")
    logger.info("=" * 70)
    logger.info(f"ğŸ“… Tarih: 2025-11-11 18:19:08 UTC")
    logger.info(f"ğŸ‘¤ KullanÄ±cÄ±: sulegogh")
    logger.info("=" * 70)

    # 1. Model yÃ¼kle
    model, model_path = load_latest_model()
    logger.info(f"âœ… Model yÃ¼klendi: {model}")

    # 2. Veri yÃ¼kle
    train_df, val_df, test_df = load_data()

    X_train = train_df.drop(columns=["koi_disposition"])
    y_train = train_df["koi_disposition"]

    X_val = val_df.drop(columns=["koi_disposition"])
    y_val = val_df["koi_disposition"]

    X_test = test_df.drop(columns=["koi_disposition"])
    y_test = test_df["koi_disposition"]

    logger.info(f"\nâœ… Veri yÃ¼klendi:")
    logger.info(f"   Train: {len(X_train):,} samples")
    logger.info(f"   Val:   {len(X_val):,} samples")
    logger.info(f"   Test:  {len(X_test):,} samples")

    # 3. Feature Importance Analizi
    importance_df = analyze_feature_importance(model, top_n=20)

    # 4. Test Set DetaylÄ± Analizi
    y_test_pred, y_test_proba, test_cm = analyze_predictions(model, X_test, y_test, "Test Set")

    # 5. Validation Set Analizi
    y_val_pred, y_val_proba, val_cm = analyze_predictions(model, X_val, y_val, "Validation Set")

    # 6. Hata Analizi (Test Set)
    analyze_errors(y_test, y_test_pred, top_n=10)

    # 7. Dataset KarÅŸÄ±laÅŸtÄ±rmasÄ±
    y_train_pred = model.predict(X_train)

    train_metrics = {
        "acc": accuracy_score(y_train, y_train_pred),
        "f1": f1_score(y_train, y_train_pred, average="weighted"),
    }
    val_metrics = {"acc": accuracy_score(y_val, y_val_pred), "f1": f1_score(y_val, y_val_pred, average="weighted")}
    test_metrics = {"acc": accuracy_score(y_test, y_test_pred), "f1": f1_score(y_test, y_test_pred, average="weighted")}

    compare_datasets(train_metrics, val_metrics, test_metrics)

    # 8. Final Ã–zet
    logger.info(f"\n" + "=" * 70)
    logger.info(f"âœ… ANALÄ°Z TAMAMLANDI!")
    logger.info("=" * 70)
    logger.info(f"ğŸ“Š Test Accuracy:  {test_metrics['acc']:.4f} ({test_metrics['acc']*100:.2f}%)")
    logger.info(f"ğŸ“Š Test F1 Score:  {test_metrics['f1']:.4f}")
    logger.info(f"ğŸ“‚ Model: {model_path.name}")
    logger.info("=" * 70)

    return 0


if __name__ == "__main__":
    sys.exit(main())
